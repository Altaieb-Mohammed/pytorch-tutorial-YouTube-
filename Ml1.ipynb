{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3Ohl3CFxapG3afuA7D8w8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Altaieb-Mohammed/pytorch-tutorial-YouTube-/blob/main/Ml1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "print(df.head(6))"
      ],
      "metadata": {
        "id": "_guc28Opvz8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxkYKBMiumYB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#---------------------------------------------------------- Лабораторная работа №1:\n",
        "\n",
        "# --- Загрузить данные из GitHub----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# --- проверка признаков----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# Использование реальных имен столбцов из нашего набора данных----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "numeric_features = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband', 'son', 'daughter',\n",
        "    'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "# ---  Чистые данные---\n",
        "# Удалить строки с отсутствующими значениями в числовых столбцах------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "df_clean = df[numeric_features].dropna()\n",
        "\n",
        "# удалить строки с неположительными значениями в финансовых столбцах.----------------------------------------------------------------------------------------------------------------------------------------------\n",
        "for col in ['debts', 'funeral_expenses', 'bequests']:\n",
        "    df_clean = df_clean[df_clean[col] > 0]\n",
        "\n",
        "# --- Масштабирование ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Min-Max Scaling\n",
        "minmax_scaler = MinMaxScaler()\n",
        "df_minmax = pd.DataFrame(\n",
        "    minmax_scaler.fit_transform(df_clean),\n",
        "    columns=numeric_features\n",
        ")\n",
        "\n",
        "# Standard Scaling------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "standard_scaler = StandardScaler()\n",
        "df_standard = pd.DataFrame(\n",
        "    standard_scaler.fit_transform(df_clean),\n",
        "    columns=numeric_features\n",
        ")\n",
        "\n",
        "# ---  результаты  ------------------------------------------------------------------------------------------------------------\n",
        "df_clean.to_csv('inheritance_cleaned.csv', index=False)\n",
        "df_minmax.to_csv('inheritance_minmax_scaled.csv', index=False)\n",
        "df_standard.to_csv('inheritance_standard_scaled.csv', index=False)\n",
        "\n",
        "# ---  Preview ---------------------------------------------------------------------------------------------  Preview------------------------------------------------------------------------------------------------------\n",
        "print(\"Cleaned data:\\n\", df_clean.head())\n",
        "print(\"MinMax scaled:\\n\", df_minmax.head())\n",
        "print(\"Standard scaled:\\n\", df_standard.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------Лабораторная работа №2: Анализ распределений и корреляций\n",
        "# ---  Загрузка данных ---\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "# ---  Определение числовых и категориальных признаков -----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Числовые признаки (финансовые и количественные)\n",
        "numerical_features = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband', 'son', 'daughter',\n",
        "    'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "# Категориальные признаки (в данном датасете только текстовое описание)--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "categorical_features = ['text']\n",
        "\n",
        "# ---  Очистка числовых данных от пропусков -----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "df_num = df[numerical_features].dropna()\n",
        "\n",
        "# ---  Анализ распределения числовых признаков -----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Гистограмма распределения--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df_num[feature], bins=30, kde=False)\n",
        "    plt.title(f'Гистограмма распределения: {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Частота')\n",
        "\n",
        "    # График плотности--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.kdeplot(df_num[feature], fill=True)\n",
        "    plt.title(f'График плотности: {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Плотность')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---  Диаграммы \"ящик с усами\" для выявления выбросов -----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=df_num[feature])\n",
        "    plt.title(f'Диаграмма \"ящик с усами\": {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.show()\n",
        "\n",
        "# --- Анализ распределения категориальных признаков -----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Для текстового поля можно вывести несколько примеров и длину текста\n",
        "print(\"\\nПримеры текстовых описаний:\")\n",
        "print(df['text'].head(5))\n",
        "df['text_length'] = df['text'].apply(len)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['text_length'], bins=30)\n",
        "plt.title('Распределение длины текстовых описаний')\n",
        "plt.xlabel('Длина текста')\n",
        "plt.ylabel('Частота')\n",
        "plt.show()\n",
        "\n",
        "# ---  Анализ взаимосвязей (корреляций) числовых признаков -----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_num.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Матрица корреляций числовых признаков')\n",
        "plt.show()\n",
        "\n",
        "# --- Краткие выводы ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "print(\"Краткие статистики по числовым признакам:\\n\", df_num.describe())\n",
        "\n",
        "# Пример анализа:\n",
        "# - Высокая корреляция между количеством сыновей и дочерей может указывать на большие семьи.\n",
        "# - Финансовые признаки (долги, расходы на похороны, завещания) могут быть скоррелированы между собой.\n",
        "# - Выбросы в признаках debts, bequests и funeral_expenses могут указывать на редкие, но крупные случаи.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Импорт библиотек\n",
        "# ------------------------------------------\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import time\n",
        "\n",
        "# ------------------------------------------\n",
        "# 1. Загрузка и предварительный анализ данных\n",
        "# ------------------------------------------\n",
        "def load_data(url):\n",
        "    try:\n",
        "        df = pd.read_csv(url)\n",
        "        print(\"Данные успешно загружены из URL.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка загрузки данных: {e}\")\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        df = pd.read_csv(next(iter(uploaded)))\n",
        "        print(\"Данные успешно загружены из локального файла.\")\n",
        "    return df\n",
        "\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "df = load_data(url)\n",
        "\n",
        "# Проверка базовой информации\n",
        "print(\"\\nИнформация о данных:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nСтатистика по пропущенным значениям:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nКоличество дубликатов:\", df.duplicated().sum())\n",
        "\n",
        "# Удаляем дубликаты, если есть\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Выбор и проверка признаков\n",
        "feature_columns = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband',\n",
        "    'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "df_clean = df[feature_columns].dropna()\n",
        "print(f\"\\nРазмер данных после удаления пропусков: {df_clean.shape}\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 2. Расширенная визуализация данных\n",
        "# ------------------------------------------\n",
        "def plot_histograms(df, columns):\n",
        "    plt.figure(figsize=(15,10))\n",
        "    for i, col in enumerate(columns, 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        sns.histplot(df[col], bins=30, kde=True)\n",
        "        plt.title(f'Распределение {col}')\n",
        "        plt.xlabel('Значение')\n",
        "        plt.ylabel('Частота')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_histograms(df_clean, ['debts', 'funeral_expenses', 'bequests'])\n",
        "\n",
        "# Матрица корреляций с маской\n",
        "plt.figure(figsize=(12,8))\n",
        "corr_matrix = df_clean.corr()\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
        "plt.title('Матрица корреляций признаков')\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3. Подготовка данных для моделирования\n",
        "# ------------------------------------------\n",
        "median_bequest = df_clean['bequests'].median()\n",
        "df_clean['high_bequest'] = (df_clean['bequests'] >= median_bequest).astype(int)\n",
        "\n",
        "# Визуализация распределения целевого признака\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='high_bequest', data=df_clean)\n",
        "plt.title('Распределение целевого признака high_bequest')\n",
        "plt.xlabel('Класс')\n",
        "plt.ylabel('Количество')\n",
        "plt.show()\n",
        "\n",
        "# Проверка баланса классов\n",
        "class_counts = df_clean['high_bequest'].value_counts()\n",
        "print(f\"\\nБаланс классов:\\n{class_counts}\")\n",
        "\n",
        "X = df_clean.drop(columns=['high_bequest', 'bequests'])\n",
        "y = df_clean['high_bequest']\n",
        "\n",
        "# Разделение данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Масштабирование\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Если классы несбалансированы, применим SMOTE для увеличения меньшего класса\n",
        "from collections import Counter\n",
        "print(f\"\\nДо SMOTE: {Counter(y_train)}\")\n",
        "if min(class_counts) / max(class_counts) < 0.5:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)\n",
        "    print(f\"После SMOTE: {Counter(y_train)}\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4. Функция для оценки модели\n",
        "# ------------------------------------------\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f'\\n{model_name} Classification Report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Матрица ошибок\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Матрица ошибок ({model_name})')\n",
        "    plt.xlabel('Предсказанный класс')\n",
        "    plt.ylabel('Истинный класс')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC-кривая и AUC\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X_test)[:,1]\n",
        "    else:\n",
        "        # Для моделей без predict_proba (например, KNN с метрикой manhattan) используем decision_function или предсказания\n",
        "        y_proba = model.predict(X_test)\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0,1], [0,1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC-кривая ({model_name})')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# 5. Обучение и сравнение моделей с кросс-валидацией\n",
        "# ------------------------------------------\n",
        "models = {\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    start_time = time.time()\n",
        "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
        "    print(f\"\\n{name} кросс-валидация F1-score: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
        "\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"{name} обучена за {end_time - start_time:.2f} секунд\")\n",
        "\n",
        "    evaluate_model(model, X_test_scaled, y_test, name)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 6. Оптимизация гиперпараметров для KNN\n",
        "# ------------------------------------------\n",
        "param_grid = {\n",
        "    'n_neighbors': range(3, 15),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "best_knn = grid_search.best_estimator_\n",
        "print(f'\\nЛучшие параметры KNN: {grid_search.best_params_}')\n",
        "\n",
        "evaluate_model(best_knn, X_test_scaled, y_test, \"KNN (optimized)\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 7. Анализ важности признаков (Random Forest)\n",
        "# ------------------------------------------\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "forest.fit(X_train_scaled, y_train)\n",
        "\n",
        "feat_importances = pd.Series(forest.feature_importances_, index=X.columns)\n",
        "feat_importances = feat_importances.sort_values(ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "feat_importances.plot(kind='barh', color='teal')\n",
        "plt.title('Важность признаков (Random Forest)')\n",
        "plt.xlabel('Важность')\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# 8. Расширенная визуализация с PCA и t-SNE\n",
        "# ------------------------------------------\n",
        "# PCA для 3D визуализации\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(X_pca[:,0], X_pca[:,1], X_pca[:,2], c=y_train, cmap='viridis', alpha=0.7)\n",
        "ax.set_title('3D визуализация PCA (финансовые данные)')\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "plt.colorbar(scatter, label='Класс')\n",
        "plt.show()\n",
        "\n",
        "# t-SNE визуализация с подписями классов\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X_train_scaled)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y_train, palette='coolwarm', alpha=0.7)\n",
        "plt.title('t-SNE визуализация распределения классов')\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.legend(title='Класс')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------------------------------- Лабораторная работа №4:\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ------------------------------------------\n",
        "# 1. Загрузка данных\n",
        "# ------------------------------------------\n",
        "try:\n",
        "    url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "    df = pd.read_csv(url)\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка загрузки данных: {e}\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "print(\"Размер данных:\", df.shape)\n",
        "print(\"Пример данных:\")\n",
        "print(df.head())\n",
        "\n",
        "# ------------------------------------------\n",
        "# 2. Выбор нужных столбцов и очистка\n",
        "# ------------------------------------------\n",
        "feature_columns = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband',\n",
        "    'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "df_selected = df[feature_columns]\n",
        "\n",
        "# Проверяем пропуски и заполняем медианой\n",
        "df_selected.fillna(df_selected.median(), inplace=True)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3. Определение целевой переменной\n",
        "# ------------------------------------------\n",
        "# Для примера создадим бинарную целевую переменную: высокие долги или нет\n",
        "median_debts = df_selected['debts'].median()\n",
        "df_selected['high_debts'] = (df_selected['debts'] >= median_debts).astype(int)\n",
        "\n",
        "X = df_selected.drop(columns=['high_debts', 'debts'])\n",
        "y = df_selected['high_debts']\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4. Разделение и масштабирование\n",
        "# ------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_std = StandardScaler()\n",
        "X_train_scaled = scaler_std.fit_transform(X_train)\n",
        "X_test_scaled = scaler_std.transform(X_test)\n",
        "\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
        "X_test_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 5. Функция для оценки выбора признаков и визуализации\n",
        "# ------------------------------------------\n",
        "def evaluate_feature_selection(score_func, score_func_name, X_train, X_test, y_train, y_test, feature_names, max_features=12):\n",
        "    feature_counts = list(range(1, min(X_train.shape[1], max_features) + 1))\n",
        "    scores = []\n",
        "    selected_features_dict = {}\n",
        "\n",
        "    for k in feature_counts:\n",
        "        selector = SelectKBest(score_func, k=k)\n",
        "        X_train_sel = selector.fit_transform(X_train, y_train)\n",
        "        X_test_sel = selector.transform(X_test)\n",
        "\n",
        "        model = RandomForestRegressor(random_state=42)\n",
        "        model.fit(X_train_sel, y_train)\n",
        "        y_pred = model.predict(X_test_sel)\n",
        "\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        scores.append(r2)\n",
        "\n",
        "        selected = [f for f, s in zip(feature_names, selector.get_support()) if s]\n",
        "        selected_features_dict[k] = selected\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(feature_counts, scores, marker='o', label=score_func_name)\n",
        "    plt.xlabel('Количество признаков')\n",
        "    plt.ylabel('R² (коэффициент детерминации)')\n",
        "    plt.title(f'Зависимость R² от количества признаков ({score_func_name})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return feature_counts, scores, selected_features_dict\n",
        "\n",
        "# ------------------------------------------\n",
        "# 6. Оценка с f_classif\n",
        "# ------------------------------------------\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "fc_counts, fc_scores, fc_features = evaluate_feature_selection(\n",
        "    f_classif, 'f_classif', X_train_scaled, X_test_scaled, y_train, y_test, feature_names\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 7. Оценка с chi2\n",
        "# ------------------------------------------\n",
        "chi_counts, chi_scores, chi_features = evaluate_feature_selection(\n",
        "    chi2, 'chi2', X_train_minmax, X_test_minmax, y_train, y_test, feature_names\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 8. Итоговая таблица результатов\n",
        "# ------------------------------------------\n",
        "results_df = pd.DataFrame({\n",
        "    'Количество признаков': fc_counts,\n",
        "    'R² (f_classif)': fc_scores,\n",
        "    'Выбранные признаки (f_classif)': [\", \".join(fc_features[k]) for k in fc_counts],\n",
        "    'R² (chi2)': chi_scores,\n",
        "    'Выбранные признаки (chi2)': [\", \".join(chi_features[k]) for k in chi_counts],\n",
        "})\n",
        "\n",
        "print(results_df.head(10))\n",
        "\n",
        "results_df.to_csv('feature_selection_results.csv', index=False)\n",
        "print(\"Результаты сохранены в 'feature_selection_results.csv'\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 9. Важность признаков с RandomForest\n",
        "# ------------------------------------------\n",
        "model_rf = RandomForestRegressor(random_state=42)\n",
        "model_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "importances = pd.Series(model_rf.feature_importances_, index=feature_names).sort_values(ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "importances.plot(kind='barh')\n",
        "plt.title('Важность признаков (Random Forest)')\n",
        "plt.xlabel('Важность')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------------------------------- Лабораторная работа №5:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    pairwise_distances,\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 1. Загрузка и подготовка данных\n",
        "# ------------------------------------------\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Выбор нужных столбцов\n",
        "feature_columns = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband',\n",
        "    'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "# Обработка пропусков и масштабирование\n",
        "X = df[feature_columns].dropna().copy()\n",
        "X.fillna(X.median(), inplace=True)  # Заполнение пропусков медианой\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 2. PCA для визуализации\n",
        "# ------------------------------------------\n",
        "pca = PCA(n_components=2)\n",
        "pca_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3. Функции для оценки кластеризации\n",
        "# ------------------------------------------\n",
        "def evaluate_clustering(X, labels):\n",
        "    if len(set(labels)) <= 1 or (set(labels) == {-1}):\n",
        "        return None, None, None\n",
        "    return (\n",
        "        silhouette_score(X, labels),\n",
        "        calinski_harabasz_score(X, labels),\n",
        "        davies_bouldin_score(X, labels)\n",
        "    )\n",
        "\n",
        "def dunn_index(X, labels):\n",
        "    unique_clusters = np.unique(labels)\n",
        "    if len(unique_clusters) <= 1 or (set(unique_clusters) == {-1}):\n",
        "        return None\n",
        "\n",
        "    distances = pairwise_distances(X)\n",
        "    intra_dists = []\n",
        "    inter_dists = []\n",
        "\n",
        "    for c in unique_clusters:\n",
        "        if c == -1: continue\n",
        "        indices = np.where(labels == c)[0]\n",
        "        if len(indices) > 1:\n",
        "            intra_dists.append(np.max(distances[np.ix_(indices, indices)]))\n",
        "\n",
        "    for i in range(len(unique_clusters)):\n",
        "        for j in range(i+1, len(unique_clusters)):\n",
        "            if -1 in (unique_clusters[i], unique_clusters[j]): continue\n",
        "            i_idx = np.where(labels == unique_clusters[i])[0]\n",
        "            j_idx = np.where(labels == unique_clusters[j])[0]\n",
        "            inter_dists.append(np.min(distances[np.ix_(i_idx, j_idx)]))\n",
        "\n",
        "    return min(inter_dists)/max(intra_dists) if intra_dists and inter_dists else None\n",
        "\n",
        "def plot_clusters(pca_data, labels, title):\n",
        "    unique_labels = set(labels) - {-1}\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for k, col in zip(unique_labels, colors):\n",
        "        mask = (labels == k)\n",
        "        plt.scatter(pca_data[mask,0], pca_data[mask,1],\n",
        "                    c=[col],\n",
        "                    label=f'Cluster {k}' if k != -1 else 'Noise',\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    sil, ch, db = evaluate_clustering(pca_data, labels)\n",
        "    metrics = f\"Silhouette: {sil:.2f} | Davies-Bouldin: {db:.2f}\" if sil else \"\"\n",
        "    plt.title(f\"{title}\\n{metrics}\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4. Кластеризация и визуализация\n",
        "# ------------------------------------------\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels_kmeans = kmeans.fit_predict(scaled_data)\n",
        "plot_clusters(pca_data, labels_kmeans, \"KMeans Clustering\")\n",
        "\n",
        "# Agglomerative\n",
        "agglo = AgglomerativeClustering(n_clusters=3)\n",
        "labels_agglo = agglo.fit_predict(scaled_data)\n",
        "plot_clusters(pca_data, labels_agglo, \"Agglomerative Clustering\")\n",
        "\n",
        "# DBSCAN (оптимизированные параметры)\n",
        "db = DBSCAN(eps=1.5, min_samples=10)\n",
        "labels_dbscan = db.fit_predict(scaled_data)\n",
        "plot_clusters(pca_data, labels_dbscan, \"DBSCAN Clustering\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 5. Сравнение метрик\n",
        "# ------------------------------------------\n",
        "metrics = {\n",
        "    'Silhouette': [\n",
        "        silhouette_score(scaled_data, labels_kmeans),\n",
        "        silhouette_score(scaled_data, labels_agglo),\n",
        "        silhouette_score(scaled_data, labels_dbscan)\n",
        "    ],\n",
        "    'Davies-Bouldin': [\n",
        "        davies_bouldin_score(scaled_data, labels_kmeans),\n",
        "        davies_bouldin_score(scaled_data, labels_agglo),\n",
        "        davies_bouldin_score(scaled_data, labels_dbscan)\n",
        "    ],\n",
        "    'Dunn Index': [\n",
        "        dunn_index(scaled_data, labels_kmeans),\n",
        "        dunn_index(scaled_data, labels_agglo),\n",
        "        dunn_index(scaled_data, labels_dbscan)\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(metrics, index=['KMeans', 'Agglomerative', 'DBSCAN'])\n",
        "print(\"\\nСводная таблица метрик:\")\n",
        "print(results_df)\n",
        "\n",
        "# Визуализация метрик\n",
        "results_df.plot(kind='bar', subplots=True, layout=(1,3), figsize=(18,5),\n",
        "                color=['skyblue', 'salmon', 'lightgreen'], edgecolor='k')\n",
        "plt.suptitle(\"Сравнение метрик кластеризации\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, OPTICS\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, pairwise_distances, calinski_harabasz_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# 1. Загрузка и подготовка данных\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "feature_columns = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband',\n",
        "    'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "X = df[feature_columns].dropna()\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# 2. Метод локтя для KMeans++\n",
        "inertias = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(k_range, inertias, marker='o')\n",
        "plt.title(\"Метод локтя для определения оптимального k\")\n",
        "plt.xlabel(\"Количество кластеров (k)\")\n",
        "plt.ylabel(\"Инерция\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Кластеризация KMeans++ (k=3)\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "df_with_clusters = df.copy()\n",
        "df_with_clusters['Cluster'] = clusters\n",
        "\n",
        "print(\"Количество объектов в каждом кластере:\")\n",
        "print(df_with_clusters['Cluster'].value_counts())\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nПримеры из кластера {i}:\")\n",
        "    print(df_with_clusters[df_with_clusters['Cluster'] == i].head(5))\n",
        "\n",
        "# 4. Визуализация кластеров с помощью t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "perplexities = [5, 30, 50, 100]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for i, p in enumerate(perplexities):\n",
        "    tsne = TSNE(n_components=2, perplexity=p, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X_scaled)\n",
        "    labels = kmeans.labels_\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=5)\n",
        "    ax.set_title(f\"t-SNE, perplexity = {p}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Агломеративная кластеризация и дендрограммы\n",
        "methods = ['complete', 'average']\n",
        "metrics = ['euclidean', 'cosine', 'cityblock']\n",
        "\n",
        "fig, axes = plt.subplots(len(methods), len(metrics), figsize=(20, 12))\n",
        "for i, method in enumerate(methods):\n",
        "    for j, metric in enumerate(metrics):\n",
        "        ax = axes[i, j]\n",
        "        Z = linkage(X_scaled, method=method, metric=metric)\n",
        "        dendrogram(Z, ax=ax, truncate_mode='lastp', p=20)\n",
        "        ax.set_title(f\"{method} - {metric}\")\n",
        "        ax.set_ylabel(\"Расстояние\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Агломеративная кластеризация (3 кластера)\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')\n",
        "clusters_agg = agg_cluster.fit_predict(X_scaled)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters_agg, cmap='Set1')\n",
        "plt.title(\"Агломеративная кластеризация (по первым двум признакам)\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 7. DBSCAN: подбор параметров и метрик\n",
        "eps_values = np.arange(0.5, 2.1, 0.1)\n",
        "min_samples_values = range(3, 11)\n",
        "metrics_list = ['euclidean', 'manhattan']\n",
        "results = []\n",
        "\n",
        "for metric in metrics_list:\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
        "            labels = db.fit_predict(X_scaled)\n",
        "            mask = labels != -1\n",
        "            n_clusters = len(set(labels[mask]))\n",
        "            n_noise = np.sum(labels == -1)\n",
        "            if n_clusters > 1:\n",
        "                silhouette = silhouette_score(X_scaled[mask], labels[mask])\n",
        "                calinski = calinski_harabasz_score(X_scaled[mask], labels[mask])\n",
        "                davies = davies_bouldin_score(X_scaled[mask], labels[mask])\n",
        "            else:\n",
        "                silhouette, calinski, davies = -1, np.nan, np.nan\n",
        "            results.append({\n",
        "                'metric': metric,\n",
        "                'eps': round(eps, 2),\n",
        "                'min_samples': min_samples,\n",
        "                'n_clusters': n_clusters,\n",
        "                'n_noise': n_noise,\n",
        "                'silhouette': silhouette,\n",
        "                'calinski': calinski,\n",
        "                'davies_bouldin': davies\n",
        "            })\n",
        "\n",
        "df_dbscan = pd.DataFrame(results)\n",
        "df_dbscan.to_csv(\"dbscan_grid_search.csv\", index=False)\n",
        "print(\"\\nТоп-10 по silhouette (DBSCAN):\")\n",
        "print(df_dbscan.sort_values(by='silhouette', ascending=False).head(10))\n",
        "\n",
        "# 8. Dunn Index\n",
        "def dunn_index(X, labels):\n",
        "    unique_clusters = np.unique(labels)\n",
        "    clusters = [np.where(labels == c)[0] for c in unique_clusters if c != -1]\n",
        "    if len(clusters) < 2:\n",
        "        return 0\n",
        "    inter_cluster_dists = []\n",
        "    for i in range(len(clusters)):\n",
        "        for j in range(i + 1, len(clusters)):\n",
        "            dist = cdist(X[clusters[i]], X[clusters[j]], metric='euclidean')\n",
        "            inter_cluster_dists.append(np.min(dist))\n",
        "    if len(inter_cluster_dists) == 0:\n",
        "        return 0\n",
        "    inter_cluster_dist = np.min(inter_cluster_dists)\n",
        "    intra_cluster_diameters = []\n",
        "    for cluster in clusters:\n",
        "        if len(cluster) > 1:\n",
        "            dists = cdist(X[cluster], X[cluster], metric='euclidean')\n",
        "            intra_cluster_diameters.append(np.max(dists))\n",
        "        else:\n",
        "            intra_cluster_diameters.append(0)\n",
        "    intra_cluster_diam = np.max(intra_cluster_diameters)\n",
        "    return inter_cluster_dist / intra_cluster_diam if intra_cluster_diam else 0\n",
        "\n",
        "# 9. Сравнение алгоритмов по метрикам\n",
        "models = {\n",
        "    \"KMeans\": KMeans(n_clusters=3, random_state=42, n_init=10),\n",
        "    \"AgglomerativeClustering\": AgglomerativeClustering(n_clusters=3),\n",
        "    \"DBSCAN\": DBSCAN(eps=0.9, min_samples=8),\n",
        "    \"OPTICS\": OPTICS(min_samples=5)\n",
        "}\n",
        "silhouette_scores = {}\n",
        "davies_bouldin_scores = {}\n",
        "dunn_indices = {}\n",
        "cluster_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    labels = model.fit_predict(X_scaled)\n",
        "    mask = labels != -1\n",
        "    silhouette_scores[name] = silhouette_score(X_scaled[mask], labels[mask]) if len(np.unique(labels[mask])) > 1 else 0\n",
        "    davies_bouldin_scores[name] = davies_bouldin_score(X_scaled[mask], labels[mask]) if len(np.unique(labels[mask])) > 1 else np.nan\n",
        "    dunn_indices[name] = dunn_index(X_scaled, labels)\n",
        "    cluster_results[name] = labels\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "axes[0].bar(silhouette_scores.keys(), silhouette_scores.values(), color=\"skyblue\")\n",
        "axes[0].set_title(\"Silhouette Score\")\n",
        "axes[1].bar(davies_bouldin_scores.keys(), davies_bouldin_scores.values(), color=\"salmon\")\n",
        "axes[1].set_title(\"Davies-Bouldin Index\")\n",
        "axes[2].bar(dunn_indices.keys(), dunn_indices.values(), color=\"lightgreen\")\n",
        "axes[2].set_title(\"Dunn Index\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}