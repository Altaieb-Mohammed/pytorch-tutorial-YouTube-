{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO0Oy9s7XmWiJNJYA0bEK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Altaieb-Mohammed/pytorch-tutorial-YouTube-/blob/main/Ml1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "print(df.head(6))"
      ],
      "metadata": {
        "id": "_guc28Opvz8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#---------------------------------------------------------- Лабораторная работа №1:\n",
        "\n",
        "\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "\n",
        "numeric_features = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband', 'son', 'daughter',\n",
        "    'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "\n",
        "df_clean = df[numeric_features].dropna()\n",
        "\n",
        "\n",
        "for col in ['debts', 'funeral_expenses', 'bequests']:\n",
        "    df_clean = df_clean[df_clean[col] > 0]\n",
        "\n",
        "\n",
        "minmax_scaler = MinMaxScaler()\n",
        "df_minmax = pd.DataFrame(\n",
        "    minmax_scaler.fit_transform(df_clean),\n",
        "    columns=numeric_features\n",
        ")\n",
        "\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "df_standard = pd.DataFrame(\n",
        "    standard_scaler.fit_transform(df_clean),\n",
        "    columns=numeric_features\n",
        ")\n",
        "\n",
        "df_clean.to_csv('inheritance_cleaned.csv', index=False)\n",
        "df_minmax.to_csv('inheritance_minmax_scaled.csv', index=False)\n",
        "df_standard.to_csv('inheritance_standard_scaled.csv', index=False)\n",
        "\n",
        "\n",
        "print(\"Cleaned data:\\n\", df_clean.head())\n",
        "print(\"MinMax scaled:\\n\", df_minmax.head())\n",
        "print(\"Standard scaled:\\n\", df_standard.head())\n"
      ],
      "metadata": {
        "id": "OxKvG5zBwepg",
        "outputId": "9827bbc9-2346-49ab-a5f0-886b442fb010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['text', 'debts', 'funeral_expenses', 'bequests', 'wife', 'husband', 'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild']\n",
            "Cleaned data:\n",
            "        debts  funeral_expenses   bequests  wife  husband  son  daughter  \\\n",
            "0  136423.38           7866.29   70554.96     1        0    5         4   \n",
            "1  178819.05           9078.99  150491.37     1        1    4         5   \n",
            "2  137791.13           6276.24   19846.02     1        1    3         5   \n",
            "3   74310.09           9243.89   81648.92     0        0    4         0   \n",
            "4    2302.81           7015.85   15827.62     1        1    3         2   \n",
            "\n",
            "   father  mother  brother  sister  grandchild  \n",
            "0       0       0        2       0           2  \n",
            "1       0       1        2       3           2  \n",
            "2       0       0        2       2           1  \n",
            "3       0       0        1       3           2  \n",
            "4       1       0        2       3           2  \n",
            "MinMax scaled:\n",
            "       debts  funeral_expenses  bequests  wife  husband  son  daughter  father  \\\n",
            "0  0.552445          0.762960  0.220081   1.0      0.0  1.0       0.8     0.0   \n",
            "1  0.724128          0.897712  0.469512   1.0      1.0  0.8       1.0     0.0   \n",
            "2  0.557983          0.586278  0.061850   1.0      1.0  0.6       1.0     0.0   \n",
            "3  0.300914          0.916035  0.254698   0.0      0.0  0.8       0.0     0.0   \n",
            "4  0.009317          0.668462  0.049312   1.0      1.0  0.6       0.4     1.0   \n",
            "\n",
            "   mother   brother    sister  grandchild  \n",
            "0     0.0  0.666667  0.000000         1.0  \n",
            "1     1.0  0.666667  1.000000         1.0  \n",
            "2     0.0  0.666667  0.666667         0.5  \n",
            "3     0.0  0.333333  1.000000         1.0  \n",
            "4     0.0  0.666667  1.000000         1.0  \n",
            "Standard scaled:\n",
            "       debts  funeral_expenses  bequests      wife   husband       son  \\\n",
            "0  1.277627          0.910157 -0.093089  0.995211 -0.998801  1.459351   \n",
            "1  2.048021          1.375589  1.147115  0.995211  1.001201  0.873007   \n",
            "2  1.302481          0.299899 -0.879832  0.995211  1.001201  0.286663   \n",
            "3  0.148934          1.438878  0.079032 -1.004812 -0.998801  0.873007   \n",
            "4 -1.159548          0.583760 -0.942177  0.995211  1.001201  0.286663   \n",
            "\n",
            "   daughter    father    mother   brother    sister  grandchild  \n",
            "0  0.897575 -1.006622 -1.007226  0.446410 -1.339769    1.228067  \n",
            "1  1.488941 -1.006622  0.992826  0.446410  1.338163    1.228067  \n",
            "2  1.488941 -1.006622 -1.007226  0.446410  0.445519    0.005623  \n",
            "3 -1.467888 -1.006622 -1.007226 -0.451257  1.338163    1.228067  \n",
            "4 -0.285157  0.993422 -1.007226  0.446410  1.338163    1.228067  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------------------------------------------------------Лабораторная работа №2: Анализ распределений и корреляций\n",
        "\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "numerical_features = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband', 'son', 'daughter',\n",
        "    'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "\n",
        "categorical_features = ['text']\n",
        "\n",
        "\n",
        "df_num = df[numerical_features].dropna()\n",
        "\n",
        "\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df_num[feature], bins=30, kde=False)\n",
        "    plt.title(f'Гистограмма распределения: {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Частота')\n",
        "\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.kdeplot(df_num[feature], fill=True)\n",
        "    plt.title(f'График плотности: {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Плотность')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=df_num[feature])\n",
        "    plt.title(f'Диаграмма \"ящик с усами\": {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"\\nПримеры текстовых описаний:\")\n",
        "print(df['text'].head(5))\n",
        "df['text_length'] = df['text'].apply(len)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['text_length'], bins=30)\n",
        "plt.title('Распределение длины текстовых описаний')\n",
        "plt.xlabel('Длина текста')\n",
        "plt.ylabel('Частота')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_num.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Матрица корреляций числовых признаков')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Краткие статистики по числовым признакам:\\n\", df_num.describe())\n",
        "\n",
        "# Пример анализа:\n",
        "# - Высокая корреляция между количеством сыновей и дочерей может указывать на большие семьи.\n",
        "# - Финансовые признаки (долги, расходы на похороны, завещания) могут быть скоррелированы между собой.\n",
        "# - Выбросы в признаках debts, bequests и funeral_expenses могут указывать на редкие, но крупные случаи.\n"
      ],
      "metadata": {
        "id": "gmRwshDHw7F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------Лабораторная работа №3:\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import time\n",
        "\n",
        "\n",
        "def load_data(url):\n",
        "    try:\n",
        "        df = pd.read_csv(url)\n",
        "        print(\"Данные успешно загружены из URL.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка загрузки данных: {e}\")\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        df = pd.read_csv(next(iter(uploaded)))\n",
        "        print(\"Данные успешно загружены из локального файла.\")\n",
        "    return df\n",
        "\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "df = load_data(url)\n",
        "\n",
        "\n",
        "print(\"\\nИнформация о данных:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nСтатистика по пропущенным значениям:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nКоличество дубликатов:\", df.duplicated().sum())\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "\n",
        "feature_columns = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband',\n",
        "    'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "df_clean = df[feature_columns].dropna()\n",
        "print(f\"\\nРазмер данных после удаления пропусков: {df_clean.shape}\")\n",
        "\n",
        "\n",
        "def plot_histograms(df, columns):\n",
        "    plt.figure(figsize=(15,10))\n",
        "    for i, col in enumerate(columns, 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        sns.histplot(df[col], bins=30, kde=True)\n",
        "        plt.title(f'Распределение {col}')\n",
        "        plt.xlabel('Значение')\n",
        "        plt.ylabel('Частота')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_histograms(df_clean, ['debts', 'funeral_expenses', 'bequests'])\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "corr_matrix = df_clean.corr()\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
        "plt.title('Матрица корреляций признаков')\n",
        "plt.show()\n",
        "\n",
        "median_bequest = df_clean['bequests'].median()\n",
        "df_clean['high_bequest'] = (df_clean['bequests'] >= median_bequest).astype(int)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='high_bequest', data=df_clean)\n",
        "plt.title('Распределение целевого признака high_bequest')\n",
        "plt.xlabel('Класс')\n",
        "plt.ylabel('Количество')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "class_counts = df_clean['high_bequest'].value_counts()\n",
        "print(f\"\\nБаланс классов:\\n{class_counts}\")\n",
        "\n",
        "X = df_clean.drop(columns=['high_bequest', 'bequests'])\n",
        "y = df_clean['high_bequest']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "print(f\"\\nДо SMOTE: {Counter(y_train)}\")\n",
        "if min(class_counts) / max(class_counts) < 0.5:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)\n",
        "    print(f\"После SMOTE: {Counter(y_train)}\")\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f'\\n{model_name} Classification Report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Матрица ошибок ({model_name})')\n",
        "    plt.xlabel('Предсказанный класс')\n",
        "    plt.ylabel('Истинный класс')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X_test)[:,1]\n",
        "    else:\n",
        "\n",
        "        y_proba = model.predict(X_test)\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0,1], [0,1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC-кривая ({model_name})')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "models = {\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    start_time = time.time()\n",
        "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
        "    print(f\"\\n{name} кросс-валидация F1-score: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
        "\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"{name} обучена за {end_time - start_time:.2f} секунд\")\n",
        "\n",
        "    evaluate_model(model, X_test_scaled, y_test, name)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': range(3, 15),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "best_knn = grid_search.best_estimator_\n",
        "print(f'\\nЛучшие параметры KNN: {grid_search.best_params_}')\n",
        "\n",
        "evaluate_model(best_knn, X_test_scaled, y_test, \"KNN (optimized)\")\n",
        "\n",
        "\n",
        "\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "forest.fit(X_train_scaled, y_train)\n",
        "\n",
        "feat_importances = pd.Series(forest.feature_importances_, index=X.columns)\n",
        "feat_importances = feat_importances.sort_values(ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "feat_importances.plot(kind='barh', color='teal')\n",
        "plt.title('Важность признаков (Random Forest)')\n",
        "plt.xlabel('Важность')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(X_pca[:,0], X_pca[:,1], X_pca[:,2], c=y_train, cmap='viridis', alpha=0.7)\n",
        "ax.set_title('3D визуализация PCA (финансовые данные)')\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "plt.colorbar(scatter, label='Класс')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X_train_scaled)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y_train, palette='coolwarm', alpha=0.7)\n",
        "plt.title('t-SNE визуализация распределения классов')\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.legend(title='Класс')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZRJPE5DhxPPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#---------------------------------------------------------- Лабораторная работа №4:\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "try:\n",
        "    url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "    df = pd.read_csv(url)\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка загрузки данных: {e}\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "print(\"Размер данных:\", df.shape)\n",
        "print(\"Пример данных:\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "feature_columns = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband',\n",
        "    'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "df_selected = df[feature_columns]\n",
        "\n",
        "df_selected.fillna(df_selected.median(), inplace=True)\n",
        "\n",
        "\n",
        "median_debts = df_selected['debts'].median()\n",
        "df_selected['high_debts'] = (df_selected['debts'] >= median_debts).astype(int)\n",
        "\n",
        "X = df_selected.drop(columns=['high_debts', 'debts'])\n",
        "y = df_selected['high_debts']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_std = StandardScaler()\n",
        "X_train_scaled = scaler_std.fit_transform(X_train)\n",
        "X_test_scaled = scaler_std.transform(X_test)\n",
        "\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
        "X_test_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "\n",
        "def evaluate_feature_selection(score_func, score_func_name, X_train, X_test, y_train, y_test, feature_names, max_features=12):\n",
        "    feature_counts = list(range(1, min(X_train.shape[1], max_features) + 1))\n",
        "    scores = []\n",
        "    selected_features_dict = {}\n",
        "\n",
        "    for k in feature_counts:\n",
        "        selector = SelectKBest(score_func, k=k)\n",
        "        X_train_sel = selector.fit_transform(X_train, y_train)\n",
        "        X_test_sel = selector.transform(X_test)\n",
        "\n",
        "        model = RandomForestRegressor(random_state=42)\n",
        "        model.fit(X_train_sel, y_train)\n",
        "        y_pred = model.predict(X_test_sel)\n",
        "\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        scores.append(r2)\n",
        "\n",
        "        selected = [f for f, s in zip(feature_names, selector.get_support()) if s]\n",
        "        selected_features_dict[k] = selected\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(feature_counts, scores, marker='o', label=score_func_name)\n",
        "    plt.xlabel('Количество признаков')\n",
        "    plt.ylabel('R² (коэффициент детерминации)')\n",
        "    plt.title(f'Зависимость R² от количества признаков ({score_func_name})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return feature_counts, scores, selected_features_dict\n",
        "\n",
        "\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "fc_counts, fc_scores, fc_features = evaluate_feature_selection(\n",
        "    f_classif, 'f_classif', X_train_scaled, X_test_scaled, y_train, y_test, feature_names\n",
        ")\n",
        "\n",
        "\n",
        "chi_counts, chi_scores, chi_features = evaluate_feature_selection(\n",
        "    chi2, 'chi2', X_train_minmax, X_test_minmax, y_train, y_test, feature_names\n",
        ")\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Количество признаков': fc_counts,\n",
        "    'R² (f_classif)': fc_scores,\n",
        "    'Выбранные признаки (f_classif)': [\", \".join(fc_features[k]) for k in fc_counts],\n",
        "    'R² (chi2)': chi_scores,\n",
        "    'Выбранные признаки (chi2)': [\", \".join(chi_features[k]) for k in chi_counts],\n",
        "})\n",
        "\n",
        "print(results_df.head(10))\n",
        "\n",
        "results_df.to_csv('feature_selection_results.csv', index=False)\n",
        "print(\"Результаты сохранены в 'feature_selection_results.csv'\")\n",
        "\n",
        "\n",
        "model_rf = RandomForestRegressor(random_state=42)\n",
        "model_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "importances = pd.Series(model_rf.feature_importances_, index=feature_names).sort_values(ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "importances.plot(kind='barh')\n",
        "plt.title('Важность признаков (Random Forest)')\n",
        "plt.xlabel('Важность')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HzlwmUysxYPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------Лабораторная работа №5:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    pairwise_distances,\n",
        ")\n",
        "\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "feature_columns = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband',\n",
        "    'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "\n",
        "X = df[feature_columns].dropna().copy()\n",
        "X.fillna(X.median(), inplace=True)\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "\n",
        "def evaluate_clustering(X, labels):\n",
        "    if len(set(labels)) <= 1 or (set(labels) == {-1}):\n",
        "        return None, None, None\n",
        "    return (\n",
        "        silhouette_score(X, labels),\n",
        "        calinski_harabasz_score(X, labels),\n",
        "        davies_bouldin_score(X, labels)\n",
        "    )\n",
        "\n",
        "def dunn_index(X, labels):\n",
        "    unique_clusters = np.unique(labels)\n",
        "    if len(unique_clusters) <= 1 or (set(unique_clusters) == {-1}):\n",
        "        return None\n",
        "\n",
        "    distances = pairwise_distances(X)\n",
        "    intra_dists = []\n",
        "    inter_dists = []\n",
        "\n",
        "    for c in unique_clusters:\n",
        "        if c == -1: continue\n",
        "        indices = np.where(labels == c)[0]\n",
        "        if len(indices) > 1:\n",
        "            intra_dists.append(np.max(distances[np.ix_(indices, indices)]))\n",
        "\n",
        "    for i in range(len(unique_clusters)):\n",
        "        for j in range(i+1, len(unique_clusters)):\n",
        "            if -1 in (unique_clusters[i], unique_clusters[j]): continue\n",
        "            i_idx = np.where(labels == unique_clusters[i])[0]\n",
        "            j_idx = np.where(labels == unique_clusters[j])[0]\n",
        "            inter_dists.append(np.min(distances[np.ix_(i_idx, j_idx)]))\n",
        "\n",
        "    return min(inter_dists)/max(intra_dists) if intra_dists and inter_dists else None\n",
        "\n",
        "def plot_clusters(pca_data, labels, title):\n",
        "    unique_labels = set(labels) - {-1}\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for k, col in zip(unique_labels, colors):\n",
        "        mask = (labels == k)\n",
        "        plt.scatter(pca_data[mask,0], pca_data[mask,1],\n",
        "                    c=[col],\n",
        "                    label=f'Cluster {k}' if k != -1 else 'Noise',\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    sil, ch, db = evaluate_clustering(pca_data, labels)\n",
        "    metrics = f\"Silhouette: {sil:.2f} | Davies-Bouldin: {db:.2f}\" if sil else \"\"\n",
        "    plt.title(f\"{title}\\n{metrics}\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels_kmeans = kmeans.fit_predict(scaled_data)\n",
        "plot_clusters(pca_data, labels_kmeans, \"KMeans Clustering\")\n",
        "\n",
        "\n",
        "agglo = AgglomerativeClustering(n_clusters=3)\n",
        "labels_agglo = agglo.fit_predict(scaled_data)\n",
        "plot_clusters(pca_data, labels_agglo, \"Agglomerative Clustering\")\n",
        "\n",
        "\n",
        "db = DBSCAN(eps=1.5, min_samples=10)\n",
        "labels_dbscan = db.fit_predict(scaled_data)\n",
        "plot_clusters(pca_data, labels_dbscan, \"DBSCAN Clustering\")\n",
        "\n",
        "metrics = {\n",
        "    'Silhouette': [\n",
        "        silhouette_score(scaled_data, labels_kmeans),\n",
        "        silhouette_score(scaled_data, labels_agglo),\n",
        "        silhouette_score(scaled_data, labels_dbscan)\n",
        "    ],\n",
        "    'Davies-Bouldin': [\n",
        "        davies_bouldin_score(scaled_data, labels_kmeans),\n",
        "        davies_bouldin_score(scaled_data, labels_agglo),\n",
        "        davies_bouldin_score(scaled_data, labels_dbscan)\n",
        "    ],\n",
        "    'Dunn Index': [\n",
        "        dunn_index(scaled_data, labels_kmeans),\n",
        "        dunn_index(scaled_data, labels_agglo),\n",
        "        dunn_index(scaled_data, labels_dbscan)\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(metrics, index=['KMeans', 'Agglomerative', 'DBSCAN'])\n",
        "print(\"\\nСводная таблица метрик:\")\n",
        "print(results_df)\n",
        "\n",
        "results_df.plot(kind='bar', subplots=True, layout=(1,3), figsize=(18,5),\n",
        "                color=['skyblue', 'salmon', 'lightgreen'], edgecolor='k')\n",
        "plt.suptitle(\"Сравнение метрик кластеризации\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U9sX0h9exfPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxkYKBMiumYB"
      },
      "outputs": [],
      "source": [
        "#---------------------------------------------------------- Лабораторная работа №6:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, OPTICS\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, pairwise_distances, calinski_harabasz_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "url = \"https://github.com/Altaieb-Mohammed/pytorch-tutorial-YouTube-/raw/main/synthetic_inheritance_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "feature_columns = [\n",
        "    'debts', 'funeral_expenses', 'bequests', 'wife', 'husband',\n",
        "    'son', 'daughter', 'father', 'mother', 'brother', 'sister', 'grandchild'\n",
        "]\n",
        "X = df[feature_columns].dropna()\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "\n",
        "inertias = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(k_range, inertias, marker='o')\n",
        "plt.title(\"Метод локтя для определения оптимального k\")\n",
        "plt.xlabel(\"Количество кластеров (k)\")\n",
        "plt.ylabel(\"Инерция\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "df_with_clusters = df.copy()\n",
        "df_with_clusters['Cluster'] = clusters\n",
        "\n",
        "print(\"Количество объектов в каждом кластере:\")\n",
        "print(df_with_clusters['Cluster'].value_counts())\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nПримеры из кластера {i}:\")\n",
        "    print(df_with_clusters[df_with_clusters['Cluster'] == i].head(5))\n",
        "\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "perplexities = [5, 30, 50, 100]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for i, p in enumerate(perplexities):\n",
        "    tsne = TSNE(n_components=2, perplexity=p, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X_scaled)\n",
        "    labels = kmeans.labels_\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=5)\n",
        "    ax.set_title(f\"t-SNE, perplexity = {p}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "methods = ['complete', 'average']\n",
        "metrics = ['euclidean', 'cosine', 'cityblock']\n",
        "\n",
        "fig, axes = plt.subplots(len(methods), len(metrics), figsize=(20, 12))\n",
        "for i, method in enumerate(methods):\n",
        "    for j, metric in enumerate(metrics):\n",
        "        ax = axes[i, j]\n",
        "        Z = linkage(X_scaled, method=method, metric=metric)\n",
        "        dendrogram(Z, ax=ax, truncate_mode='lastp', p=20)\n",
        "        ax.set_title(f\"{method} - {metric}\")\n",
        "        ax.set_ylabel(\"Расстояние\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')\n",
        "clusters_agg = agg_cluster.fit_predict(X_scaled)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters_agg, cmap='Set1')\n",
        "plt.title(\"Агломеративная кластеризация (по первым двум признакам)\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "eps_values = np.arange(0.5, 2.1, 0.1)\n",
        "min_samples_values = range(3, 11)\n",
        "metrics_list = ['euclidean', 'manhattan']\n",
        "results = []\n",
        "\n",
        "for metric in metrics_list:\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
        "            labels = db.fit_predict(X_scaled)\n",
        "            mask = labels != -1\n",
        "            n_clusters = len(set(labels[mask]))\n",
        "            n_noise = np.sum(labels == -1)\n",
        "            if n_clusters > 1:\n",
        "                silhouette = silhouette_score(X_scaled[mask], labels[mask])\n",
        "                calinski = calinski_harabasz_score(X_scaled[mask], labels[mask])\n",
        "                davies = davies_bouldin_score(X_scaled[mask], labels[mask])\n",
        "            else:\n",
        "                silhouette, calinski, davies = -1, np.nan, np.nan\n",
        "            results.append({\n",
        "                'metric': metric,\n",
        "                'eps': round(eps, 2),\n",
        "                'min_samples': min_samples,\n",
        "                'n_clusters': n_clusters,\n",
        "                'n_noise': n_noise,\n",
        "                'silhouette': silhouette,\n",
        "                'calinski': calinski,\n",
        "                'davies_bouldin': davies\n",
        "            })\n",
        "\n",
        "df_dbscan = pd.DataFrame(results)\n",
        "df_dbscan.to_csv(\"dbscan_grid_search.csv\", index=False)\n",
        "print(\"\\nТоп-10 по silhouette (DBSCAN):\")\n",
        "print(df_dbscan.sort_values(by='silhouette', ascending=False).head(10))\n",
        "\n",
        "# 8. Dunn Index\n",
        "def dunn_index(X, labels):\n",
        "    unique_clusters = np.unique(labels)\n",
        "    clusters = [np.where(labels == c)[0] for c in unique_clusters if c != -1]\n",
        "    if len(clusters) < 2:\n",
        "        return 0\n",
        "    inter_cluster_dists = []\n",
        "    for i in range(len(clusters)):\n",
        "        for j in range(i + 1, len(clusters)):\n",
        "            dist = cdist(X[clusters[i]], X[clusters[j]], metric='euclidean')\n",
        "            inter_cluster_dists.append(np.min(dist))\n",
        "    if len(inter_cluster_dists) == 0:\n",
        "        return 0\n",
        "    inter_cluster_dist = np.min(inter_cluster_dists)\n",
        "    intra_cluster_diameters = []\n",
        "    for cluster in clusters:\n",
        "        if len(cluster) > 1:\n",
        "            dists = cdist(X[cluster], X[cluster], metric='euclidean')\n",
        "            intra_cluster_diameters.append(np.max(dists))\n",
        "        else:\n",
        "            intra_cluster_diameters.append(0)\n",
        "    intra_cluster_diam = np.max(intra_cluster_diameters)\n",
        "    return inter_cluster_dist / intra_cluster_diam if intra_cluster_diam else 0\n",
        "\n",
        "\n",
        "models = {\n",
        "    \"KMeans\": KMeans(n_clusters=3, random_state=42, n_init=10),\n",
        "    \"AgglomerativeClustering\": AgglomerativeClustering(n_clusters=3),\n",
        "    \"DBSCAN\": DBSCAN(eps=0.9, min_samples=8),\n",
        "    \"OPTICS\": OPTICS(min_samples=5)\n",
        "}\n",
        "silhouette_scores = {}\n",
        "davies_bouldin_scores = {}\n",
        "dunn_indices = {}\n",
        "cluster_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    labels = model.fit_predict(X_scaled)\n",
        "    mask = labels != -1\n",
        "    silhouette_scores[name] = silhouette_score(X_scaled[mask], labels[mask]) if len(np.unique(labels[mask])) > 1 else 0\n",
        "    davies_bouldin_scores[name] = davies_bouldin_score(X_scaled[mask], labels[mask]) if len(np.unique(labels[mask])) > 1 else np.nan\n",
        "    dunn_indices[name] = dunn_index(X_scaled, labels)\n",
        "    cluster_results[name] = labels\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "axes[0].bar(silhouette_scores.keys(), silhouette_scores.values(), color=\"skyblue\")\n",
        "axes[0].set_title(\"Silhouette Score\")\n",
        "axes[1].bar(davies_bouldin_scores.keys(), davies_bouldin_scores.values(), color=\"salmon\")\n",
        "axes[1].set_title(\"Davies-Bouldin Index\")\n",
        "axes[2].bar(dunn_indices.keys(), dunn_indices.values(), color=\"lightgreen\")\n",
        "axes[2].set_title(\"Dunn Index\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}